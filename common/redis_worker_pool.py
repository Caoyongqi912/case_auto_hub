#!/usr/bin/env python# -*- coding:utf-8 -*-# @Time : 2025/12/8# @Author : cyq# @File : redis_worker_pool# @Software: PyCharm# @Desc:import asyncioimport jsonimport picklefrom enum import Enumfrom typing import Optional, Dict, List, Callable, Any, Unionfrom datetime import datetimefrom objc import object_propertyfrom common import RedisClientfrom dataclasses import dataclass, field, asdictimport timeimport uuidfrom config import Configfrom utils import MyLogurulog = MyLoguru().get_logger()def generate_server_id() -> str:    """生成服务器唯一ID"""    import socket    hostname = socket.gethostname()    timestamp = int(time.time())    return f"{hostname}_{timestamp}"class JobStatus(Enum):    """任务状态"""    PENDING = "pending"  # 等待中    RUNNING = "running"  # 执行中    COMPLETED = "completed"  # 已完成    FAILED = "failed"  # 失败    CANCELLED = "cancelled"  # 已取消@dataclassclass JOB:    """    任务对象    """    id: str    name: str    func_name: str  # 函数名称，用于反序列化时查找    args: tuple = field(default_factory=tuple)    kwargs: dict = field(default_factory=dict)    status: JobStatus = JobStatus.PENDING    result: Optional[Any] = None    error: Optional[str] = None    start_time: Optional[float] = None    end_time: Optional[float] = None    created_at: float = field(default_factory=time.time)    worker_id: Optional[str] = None  # 执行此任务的worker ID    server_id: Optional[str] = None  # 服务器ID    def __post_init__(self):        if not self.id:            self.id = str(uuid.uuid4())[:8]    @property    def duration(self) -> Optional[float]:        """执行时长"""        if self.start_time and self.end_time:            return self.end_time - self.start_time        return None    def to_dict(self) -> dict:        """转换为字典（可序列化）"""        data = asdict(self)        data['status'] = self.status.value        # 处理特殊类型        data['args'] = self._serialize_args(self.args)        data['kwargs'] = self._serialize_kwargs(self.kwargs)        return data    @classmethod    def from_dict(cls, data: dict) -> 'JOB':        """从字典创建"""        data = data.copy()        data['status'] = JobStatus(data['status'])        data['args'] = cls._deserialize_args(data['args'])        data['kwargs'] = cls._deserialize_kwargs(data['kwargs'])        return cls(**data)    @staticmethod    def _serialize_args(args: tuple) -> str:        """序列化参数"""        return pickle.dumps(args).hex()    @staticmethod    def _deserialize_args(args_hex: str) -> tuple:        """反序列化参数"""        return pickle.loads(bytes.fromhex(args_hex))    @staticmethod    def _serialize_kwargs(kwargs: dict) -> str:        """序列化关键字参数"""        return pickle.dumps(kwargs).hex()    @staticmethod    def _deserialize_kwargs(kwargs_hex: str) -> dict:        """反序列化关键字参数"""        return pickle.loads(bytes.fromhex(kwargs_hex))class RedisWorkerPool:    """    redis任务池    -  任务队列 job_queue:{queue_name}    -  处理中 job_processing    -  任务数据 job_data    -  任务结果 job_results    -  死亡任务队列 job_dead_letter    """    _instances: Dict[str, 'RedisWorkerPool'] = {}    redis_client: Optional["RedisClient"] = None    @classmethod    def get_instance(cls,                     queue_name: str = "default",                     worker_count: int = 5,                     server_id: str = None) -> 'RedisWorkerPool':        """获取单例实例（按队列名区分）"""        instance_key = f"{queue_name}_{server_id or 'default'}"        if instance_key not in cls._instances:            cls._instances[instance_key] = cls(                queue_name=queue_name,                worker_count=worker_count,                server_id=server_id            )        return cls._instances[instance_key]    def __init__(self, queue_name: str = "default", worker_count: int = 5, server_id: str = None):        self.queue_name = queue_name        self.worker_count = worker_count        self.server_id = server_id or generate_server_id()        self.redis_client = RedisClient()        # Redis keys        self.queue_key = f"job_queue:{queue_name}"  # 任务队列        self.processing_key = f"job_processing:{queue_name}"  # 处理中的任务        self.job_data_key = f"job_data:{queue_name}"  # 任务数据哈希表        self.results_key = f"job_results:{queue_name}"  # 任务结果哈希表        self.dead_letter_key = f"job_dead_letter:{queue_name}"  # 死信队列        # 本地状态        self.workers: List[asyncio.Task] = []        self.local_jobs: Dict[str, JOB] = {}  # 本服务器处理过的任务        self.is_running = False        self.monitor_task: Optional[asyncio.Task] = None        # 函数注册表（用于反序列化）        self.function_registry: Dict[str, Callable] = {}        log.info(            f"RedisWorkerPool init redis worker pool : Queue Name={queue_name}  workers={worker_count}, server={self.server_id}")    @property    def redis(self):        if self.redis_client is None:            raise RuntimeError("miss redis")        return self.redis_client.r    def register_function(self, func: Callable = None)        """注册函数（装饰器或直接调用）"""        def decorator(f: Callable):            func_name = f.__name__            self.function_registry[func_name] = f            log.info(f"RedisWorkerPool register function: {func_name}")            log.info(f"RedisWorkerPool register function map: {self.function_registry}")            return f        if func is None:            return decorator        return decorator(func)    async def start(self):        """        启动工作池        - 初始化redis        - worker        - monitor        """        if self.is_running:            log.warning("redis worker pool already running ..... ")            return        try:            await self.connect_redis()        except Exception as e:            log.error(f"start fail ，connect redis fail: {e}")            raise        self.is_running = True        # 启动工作协程        self.workers = [            asyncio.create_task(self._worker(i), name=f"RedisWorker-{i}")            for i in range(self.worker_count)        ]        # 启动监控任务        self.monitor_task = asyncio.create_task(self._monitor(), name="RedisMonitor")        log.info(            f"====================== Redis工作池启动: {self.worker_count} workers, server={self.server_id} ====================== ")    async def stop(self):        """停止工作池"""        pass    async def connect_redis(self):        """        连接redis        """        await self.redis_client.set_pool(            host=Config.REDIS_SERVER,            port=Config.REDIS_PORT,            db=Config.REDIS_WORKER_POOL_BD,            decode_responses=False,            max_connections=100)    async def disconnect_redis(self):        """断开Redis连接"""        if self.redis_client:            await  self.redis_client.r.close()            self.redis_client = None            log.info("RedisWorkerPool 连 接 已 关 闭")    async def submit_to_redis(self, func: Callable, name: str, args: tuple = (), kwargs: dict = None,                              priority: int = 0) -> str:        """        添加任务        保存任务数据 && 添加任务到队列        :params func        :params name        :params args        :params kwargs        :params priority 优先级        """        # 确认 redis 连接        await self.connect_redis()        if not kwargs:            kwargs = {}        func_name = func.__name__        job = JOB(id=str(uuid.uuid4()), name=name, func_name=func_name, args=args, kwargs=kwargs, )        # 序列化任务        job_data = job.to_dict()        serialized_job = pickle.dumps(job_data)        log.info(            f"submit_to_redis: save job data to redis {job_data}: redis name={self.job_data_key} key={job.id}")        # 保存任务数据到Redis哈希表        await self.redis.hset(            name=self.job_data_key,            key=job.id,            value=serialized_job,        )        # 计算优先级分数（时间戳 + 优先级）        # 优先级越高，分数越小，越先被处理        score = time.time() + priority * 1000        # 添加到有序集合（实现优先级队列）        await self.redis.zadd(            self.queue_key,            {job.id: score}        )        log.info(            f"submit_to_redis: add job to redis Queue : queue_key = {self.queue_key} job_id = {job.id} score={score}")        # 本地记录        self.local_jobs[job.id] = job        # 获取队列长度        queue_size = await self.redis.zcard(self.queue_key)        log.info(f"提交任务 {job.id}: {name} "                 f"(优先级: {priority}, 队列: {queue_size})")        return job.id    async def _worker(self, worker_id: int):        """        任务        """        worker_name = f"{self.server_id}_worker_{worker_id}"        try:            await self.connect_redis()            log.info(f" worker:【{worker_name}】 start ， connect to redis success")        except Exception as e:            log.error(f"{worker_name} connect redis fail: {e}")            return        while self.is_running:            try:                # 获取任务                result = await self.redis.bzpopmin(self.queue_key, timeout=1)                if not result:                    continue                log.info(f"worker:【{worker_name}】 get JOB: {result}")                _key, job_id, _score = result  # (key, value)                job_id = job_id.decode('utf-8')                log.info(f"worker:【{worker_name}】 get JOB decode: {job_id}")                # 从哈希表获取任务数据                serialized_job = await self.redis.hget(self.job_data_key, job_id)                if not serialized_job:                    log.error(f"{worker_name} 找不到任务数据: {job_id}")                    continue                    # 反序列化任务                job_data = pickle.loads(serialized_job)                job = JOB.from_dict(job_data)                # 标记任务为处理中（移动到处理中集合）                processing_key = f"{self.processing_key}:{worker_name}"                await self.redis.hset(                    key=processing_key,                    name=job_id,                    value=serialized_job                )                await self.__job_running(job, worker_name)                log.info(f"{worker_name} 开始处理任务 {job.id}: {job.name}")                await self.__worker_execute_job(job, processing_key)            except asyncio.CancelledError:                log.info(f"{worker_name} 被取消")                break            except Exception as e:                log.exception(f"{worker_name} 异常: {e}")                await asyncio.sleep(1)  # 避免疯狂重试                log.info(f"{worker_name} 停止")    async def _monitor(self):        """        监控        """        try:            log.info("[RedisMonitor] 启动")            while self.is_running:                await asyncio.sleep(10)  # 每10秒监控一次                try:                    stats = await self.get_stats()                    log.info(                        f"[RedisMonitor] 队列: {stats['queue_size']}, "                        f"处理中: {stats['processing']}, "                        f"已完成: {stats['completed']}, "                        f"失败: {stats['failed']}, "                        f"服务器: {stats['servers']}"                    )                except Exception as e:                    log.error(f"[RedisMonitor] 获取统计失败: {e}")        except asyncio.CancelledError:            log.info("[RedisMonitor] 停止")    async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:        """获取任务状态"""        try:            await self.connect_redis()            # 从Redis获取任务数据            serialized_job = await self.redis.hget(self.job_data_key, job_id)            if not serialized_job:                return None            job_data = pickle.loads(serialized_job)            job = JOB.from_dict(job_data)            # 获取结果（如果有）            result_data = await self.redis.hget(self.results_key, job_id)            if result_data:                result_json = json.loads(result_data)                if 'result' in result_json:                    job.result = pickle.loads(bytes.fromhex(result_json['result']))            return {                'id': job.id,                'name': job.name,                'status': job.status.value,                'created_at': datetime.fromtimestamp(job.created_at).isoformat(),                'start_time': datetime.fromtimestamp(job.start_time).isoformat() if job.start_time else None,                'end_time': datetime.fromtimestamp(job.end_time).isoformat() if job.end_time else None,                'duration': job.duration,                'result': job.result,                'error': job.error,                'worker_id': job.worker_id,                'server_id': job.server_id            }        except Exception as e:            log.error(f"获取任务状态失败 {job_id}: {e}")            return None    async def get_stats(self) -> Dict[str, Any]:        """获取统计信息"""        try:            await self.connect_redis()            # 队列大小            queue_size = await self.redis.zcard(self.queue_key)            # 处理中的任务（所有服务器）            processing_keys = await self.redis.keys(f"{self.processing_key}:*")            processing = 0            for key in processing_keys:                count = await self.redis.hlen(key)                processing += count            # 已完成和失败的任务            results = await self.redis.hgetall(self.results_key)            completed = 0            failed = 0            for result_json in results.values():                result = json.loads(result_json)                if result.get('status') == 'completed':                    completed += 1                else:                    failed += 1            # 活跃的服务器（最近5分钟有活动的）            servers = set()            for key in processing_keys:                # key格式: job_processing:queue_name:server_worker_X                parts = key.decode('utf-8').split(':')                if len(parts) >= 3:                    server_worker = parts[2]                    server_id = '_'.join(server_worker.split('_')[:-2])  # 去掉worker部分                    servers.add(server_id)            return {                'queue_name': self.queue_name,                'queue_size': queue_size,                'processing': processing,                'completed': completed,                'failed': failed,                'dead_letter': await self.redis.llen(self.dead_letter_key),                'servers': list(servers),                'server_id': self.server_id,                'local_workers': self.worker_count            }        except Exception as e:            log.error(f"获取统计失败: {e}")            return {}    async def cancel_job(self, job_id: str) -> bool:        """取消任务（如果还在队列中）"""        try:            await self.connect_redis()            # 从队列中移除            removed = await self.redis.zrem(self.queue_key, job_id)            if removed:                # 标记为取消                serialized_job = await self.redis.hget(self.job_data_key, job_id)                if serialized_job:                    job_data = pickle.loads(serialized_job)                    job = JOB.from_dict(job_data)                    job.status = JobStatus.CANCELLED                    job.end_time = time.time()                    await self.redis.hset(                        self.job_data_key,                        job_id,                        pickle.dumps(job.to_dict())                    )                log.info(f"取消任务: {job_id}")                return True            return False        except Exception as e:            log.error(f"取消任务失败 {job_id}: {e}")            return False    async def retry_failed_jobs(self, limit: int = 10) -> List[str]:        """重试失败的任务（从死信队列）"""        try:            await self.connect_redis()            retried_jobs = []            for _ in range(limit):                # 从死信队列获取一个失败任务                error_data = await self.redis.rpop(self.dead_letter_key)                if not error_data:                    break                error_info = json.loads(error_data)                job_data = error_info.get('job_data')                if job_data:                    # 重新提交任务                    job = JOB.from_dict(job_data)                    job.id = str(uuid.uuid4())  # 新的ID                    job.status = JobStatus.PENDING                    job.start_time = None                    job.end_time = None                    job.error = None                    # 重新提交到队列                    serialized_job = pickle.dumps(job.to_dict())                    await self.redis.hset(                        self.job_data_key,                        job.id,                        serialized_job                    )                    score = time.time()                    await self.redis.zadd(self.queue_key, {job.id: score})                    retried_jobs.append(job.id)                    log.info(f"重试任务: {job.id} (原任务: {job_data['id']})")            log.info(f"重试了 {len(retried_jobs)} 个失败任务")            return retried_jobs        except Exception as e:            log.error(f"重试失败任务错误: {e}")            return []    async def cleanup_old_jobs(self, older_than_hours: int = 24):        """清理旧的任务数据"""        try:            await self.connect_redis()            cutoff_time = time.time() - (older_than_hours * 3600)            # 获取所有任务            all_jobs = await self.redis.hgetall(self.job_data_key)            deleted = 0            for job_id, serialized_job in all_jobs.items():                job_id = job_id.decode('utf-8')                job_data = pickle.loads(serialized_job)                job = JOB.from_dict(job_data)                # 删除超过指定时间的已完成或失败任务                if job.end_time and job.end_time < cutoff_time:                    if job.status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.CANCELLED]:                        # 从各个存储中删除                        await self.redis.hdel(self.job_data_key, job_id)                        await self.redis.hdel(self.results_key, job_id)                        # 从处理中集合删除（如果存在）                        processing_keys = await self.redis.keys(f"{self.processing_key}:*")                        for key in processing_keys:                            await self.redis.hdel(key, job_id)                        deleted += 1            log.info(f"清理了 {deleted} 个旧任务")            return deleted        except Exception as e:            log.error(f"清理旧任务错误: {e}")            return 0    async def __worker_execute_job(self, job: JOB, processing_key: str):        """        任务执行        """        try:            if job.func_name not in self.function_registry:                raise ValueError(f"找不到函数: {job.func_name}")            _func = self.function_registry[job.func_name]            _func_result = await _func(*job.args, **job.kwargs)            await self.__job_completed(job, _func_result)            log.info(f"任务 {job}  执行完成 result: {_func_result} "                     f"耗时 {job.duration:.2f}s")        except asyncio.CancelledError:            # 任务被取消            job.status = JobStatus.CANCELLED            job.end_time = time.time()            raise        except Exception as e:            await self.__job_field(job, e)            log.error(f"任务执行失败: {e}")        finally:            final_job_data = job.to_dict()            await self.redis.hset(                self.job_data_key,                job.id,                pickle.dumps(final_job_data)            )            # 从处理中集合移除            await self.redis.hdel(processing_key, job.id)            # 本地记录            self.local_jobs[job.id] = job    async def __job_running(self, job: JOB, worker_name: str):        """        任务运行中        """        # 更新任务状态        job.status = JobStatus.RUNNING        job.start_time = time.time()        job.worker_id = worker_name        # 保存更新后的任务状态        updated_job_data = job.to_dict()        await self.redis.hset(            key=self.job_data_key,            name=job.id,            value=pickle.dumps(updated_job_data)        )    async def __job_completed(self, job: JOB, result: dict[str, any]):        """        - 设置任务状态        - 保存结果        """        job.status = JobStatus.COMPLETED        job.result = result        job.end_time = time.time()        # 保存结果        result_data = {            'status': 'completed',            'result': pickle.dumps(result).hex(),            'end_time': job.end_time,            'duration': job.duration        }        await self.redis.hset(            self.results_key,            job.id,            json.dumps(result_data)        )    async def __job_field(self, job: JOB, e: Exception):        """        任务失败        """        job.status = JobStatus.FAILED        job.error = str(e)        job.end_time = time.time()        error_data = {            "error": str(e),            "failed_at": time.time(),            "job_data": job.to_dict()        }        await self.redis.lpush(            self.dead_letter_key,            json.dumps(error_data)        )r_pool = RedisWorkerPool.get_instance()# 注册任务函数（两台服务器都要注册相同的函数）@r_pool.register_functionasync def process_api_request(api_name: str, params: dict):    """处理API请求"""    print(f"处理API: {api_name}, 参数: {params}")    await asyncio.sleep(2)    return {"status": "success", "api": api_name}@r_pool.register_functionasync def sync_data(source: str, target: str):    """数据同步任务"""    print(f"同步数据: {source} -> {target}")    await asyncio.sleep(3)    return {"synced": True, "source": source, "target": target}async def main():    await r_pool.start()    try:        # 服务器1提交任务        job_id1 = await r_pool.submit_to_redis(            func=process_api_request,            name="用户API",            args=("user_api",),            kwargs={"params": {"action": "get_users"}},            priority=5        )        job_id2 = await r_pool.submit_to_redis(            func=sync_data,            name="数据库同步",            args=("mysql", "elasticsearch"),            priority=3        )        print(f"提交的任务ID: {job_id1}, {job_id2}")        # 监控一段时间        for i in range(10):            stats = await r_pool.get_stats()            print(f"状态[{i}]: 队列={stats['queue_size']}, "                  f"处理中={stats['processing']}, 服务器={stats['servers']}")            await asyncio.sleep(2)        # 获取任务状态        status = await r_pool.get_job_status(job_id1)        print(f"任务状态: {status}")    finally:        await r_pool.stop()if __name__ == '__main__':    asyncio.run(main())